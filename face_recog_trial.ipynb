{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of face_recog.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "356069b3f7b249cf92928fa84ba2becd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_79d0c26cccfc4d24b1a949ae261e506c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_52c9a19794ea47d6b64242cab9fd0f06",
              "IPY_MODEL_04d0e96d47264dbab7067ef62a96ef5e"
            ]
          }
        },
        "79d0c26cccfc4d24b1a949ae261e506c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52c9a19794ea47d6b64242cab9fd0f06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a0624908827146f6a715d3342c7066d4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "danger",
            "max": 9,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 8,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab2de319f2a349ec99aea1ea10bdb4de"
          }
        },
        "04d0e96d47264dbab7067ef62a96ef5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d336653a687d438089eb0531b8c9dde3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 89% 8/9 [00:33&lt;00:05,  5.78s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e8777a612114c36b6c22d2f89cae751"
          }
        },
        "a0624908827146f6a715d3342c7066d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab2de319f2a349ec99aea1ea10bdb4de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d336653a687d438089eb0531b8c9dde3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e8777a612114c36b6c22d2f89cae751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2f79ab544bd4135994afb209de9737e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0a4481f2cc394aa081b77f2491c65696",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f267b444ad1b40a7808ce76ec34f56bd",
              "IPY_MODEL_a2c1e92248da4685a26249289cedc117"
            ]
          }
        },
        "0a4481f2cc394aa081b77f2491c65696": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f267b444ad1b40a7808ce76ec34f56bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_806b2dca20b64d869c75aae5fcfa3018",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "danger",
            "max": 57,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 24,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b31a9f4d8d4a4475a9e03e60c39fb5a2"
          }
        },
        "a2c1e92248da4685a26249289cedc117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4af6d3143e3d4cfb8c3a330f594030fc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 42% 24/57 [02:18&lt;03:22,  6.14s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db69991cf7c14d0dbc04ccb758f6cc40"
          }
        },
        "806b2dca20b64d869c75aae5fcfa3018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b31a9f4d8d4a4475a9e03e60c39fb5a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4af6d3143e3d4cfb8c3a330f594030fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db69991cf7c14d0dbc04ccb758f6cc40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdyGjQS5YXc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://towardsdatascience.com/facial-recognition-using-deep-learning-a74e9059a150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QE_swn24luq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dlib\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib.pyplot import imread\n",
        "import imageio\n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "opSHGIcOK2ls",
        "outputId": "5265aee0-8c81-42b6-b6be-b8bec06a1686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0VQYLtYWLXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install scipy==1.4.1\n",
        "# scipy.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n14AwTl-THoN",
        "colab_type": "code",
        "outputId": "6aa55821-f255-4f41-a9a2-1d81131617e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "himanshu  obama  saanika\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g24yfqKJpMv",
        "colab_type": "code",
        "outputId": "f4d669ff-f38a-4d44-de21-01a84fa46e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/gdrive/My Drive/face_recognition/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/face_recognition\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pudzmptSMCCO",
        "colab_type": "code",
        "outputId": "88c1983a-5937-4565-cd98-0775c5a7c2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " dlib_face_recognition_resnet_model_v1.dat\n",
            " DSC_1907.jpg\n",
            " DSC_1915.jpg\n",
            " DSC_1921.jpg\n",
            " face_recog.ipynb\n",
            " \u001b[0m\u001b[01;34mimages\u001b[0m/\n",
            " shape_predictor_68_face_landmarks.dat\n",
            " \u001b[01;34mtest\u001b[0m/\n",
            " \u001b[01;34mtrain\u001b[0m/\n",
            "'WhatsApp Image 2020-02-08 at 1.32.08 AM.jpg'\n",
            "'WhatsApp Image 2020-02-08 at 1.32.34 AM.jpg'\n",
            "'WhatsApp Image 2020-02-08 at 1.33.17 AM.jpg'\n",
            "'WhatsApp Image 2020-02-08 at 1.33.32 AM.jpg'\n",
            "'WhatsApp Image 2020-02-08 at 1.35.48 AM.jpg'\n",
            "'WhatsApp Image 2020-02-08 at 1.38.16 AM.jpg'\n",
            "'WhatsApp Image 2020-02-08 at 1.38.28 AM.jpg'\n",
            "'WhatsApp Image 2020-02-08 at 1.40.09 AM.jpg'\n",
            " ZDSC_1908.jpg\n",
            " ZDSC_1909.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKcXYmBGCcBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Face Detector from dlib\n",
        "# This allows us to detect faces in images\n",
        "face_detector = dlib.get_frontal_face_detector()\n",
        "# Get Pose Predictor from dlib\n",
        "# This allows us to detect landmark points in faces and understand the pose/angle of the face\n",
        "shape_predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
        "# Get the face recognition model\n",
        "# This is what gives us the face encodings (numbers that identify the face of a particular person)\n",
        "face_recognition_model = dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat')\n",
        "# This is the tolerance for face comparisons\n",
        "# The lower the number - the stricter the comparison\n",
        "# To avoid false matches, use lower value\n",
        "# To avoid false negatives (i.e. faces of the same person doesn't match), use higher value\n",
        "# 0.5-0.6 works well\n",
        "TOLERANCE = 0.4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RFm50aYCh5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function will take an image and return its face encodings using the neural network\n",
        "def get_face_encodings(path_to_image):\n",
        "    # Load image using scipy\n",
        "    image = imageio.imread(path_to_image)\n",
        "    # Detect faces using the face detector\n",
        "    detected_faces = face_detector(image, 1)\n",
        "    # Get pose/landmarks of those faces\n",
        "    # Will be used as an input to the function that computes face encodings\n",
        "    # This allows the neural network to be able to produce similar numbers for faces of the same people, regardless of camera angle and/or face positioning in the image\n",
        "    shapes_faces = [shape_predictor(image, face) for face in detected_faces]\n",
        "    # For every face detected, compute the face encodings\n",
        "    return [np.array(face_recognition_model.compute_face_descriptor(image, face_pose, 1)) for face_pose in shapes_faces]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttn3M1ZSCo9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function takes a list of known faces\n",
        "def compare_face_encodings(known_faces, face):\n",
        "    # Finds the difference between each known face and the given face (that we are comparing)\n",
        "    # Calculate norm for the differences with each known face\n",
        "    # Return an array with True/Face values based on whether or not a known face matched with the given face\n",
        "    # A match occurs when the (norm) difference between a known face and the given face is less than or equal to the TOLERANCE value\n",
        "    return (np.linalg.norm(known_faces - face, axis=1) <= TOLERANCE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm9Dj_BcMbz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function returns the name of the person whose image matches with the given face (or 'Not Found')\n",
        "# known_faces is a list of face encodings\n",
        "# names is a list of the names of people (in the same order as the face encodings - to match the name with an encoding)\n",
        "# face is the face we are looking for\n",
        "def find_match(known_faces, names, face):\n",
        "    # Call compare_face_encodings to get a list of True/False values indicating whether or not there's a match\n",
        "    matches = compare_face_encodings(known_faces, face)\n",
        "    # Return the name of the first match\n",
        "    count = 0\n",
        "    for match in matches:\n",
        "        if match:\n",
        "            # print(\"names=\",names)\n",
        "            return names[count].split('/')[1]\n",
        "            \n",
        "        count += 1\n",
        "    # Return not found if no match found\n",
        "    return 'Not Found'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO9wCjAUMd9R",
        "colab_type": "code",
        "outputId": "7a985635-61a6-47c6-9144-500f3df7a094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477,
          "referenced_widgets": [
            "356069b3f7b249cf92928fa84ba2becd",
            "79d0c26cccfc4d24b1a949ae261e506c",
            "52c9a19794ea47d6b64242cab9fd0f06",
            "04d0e96d47264dbab7067ef62a96ef5e",
            "a0624908827146f6a715d3342c7066d4",
            "ab2de319f2a349ec99aea1ea10bdb4de",
            "d336653a687d438089eb0531b8c9dde3",
            "6e8777a612114c36b6c22d2f89cae751"
          ]
        }
      },
      "source": [
        "%%time\n",
        "# Get path to all the known images\n",
        "# Filtering on .jpg extension - so this will only work with JPEG images ending with .jpg\n",
        "face_encodings = []\n",
        "names = []\n",
        "my_list =['saanika','himanshu']\n",
        "for z in my_list:\n",
        "    image_filenames = filter(lambda x: x.endswith('.jpg'), os.listdir('/content/gdrive/My Drive/face_recognition/train/' + z + '/'))\n",
        "    # Sort in alphabetical order\n",
        "    image_filenames = sorted(image_filenames)\n",
        "    # Get full paths to images\n",
        "    paths_to_images = ['train/' + z + '/' + x for x in image_filenames]\n",
        "    # List of face encodings we have\n",
        "    print(paths_to_images)\n",
        "    # Loop over images to get the encoding one by one\n",
        "    for path_to_image in tqdm_notebook(paths_to_images):\n",
        "        \n",
        "        # Get face encodings from the image\n",
        "        f = 0\n",
        "        face_encodings_in_image = get_face_encodings(path_to_image)\n",
        "        # Make sure there's exactly one face in the image\n",
        "        if len(face_encodings_in_image) != 1:\n",
        "            print(\"Please change image: \" + path_to_image + \" - it has \" + str(len(face_encodings_in_image)) + \" faces; it can only have one\")\n",
        "            f = 1\n",
        "            # exit()\n",
        "        # Append the face encoding found in that image to the list of face encodings we have\n",
        "        if(f == 0):\n",
        "            names.append(path_to_image)\n",
        "            face_encodings.append(get_face_encodings(path_to_image)[0])\n",
        "            # face_encodings.append(face_encodings_in_image(path_to_image)[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['train/saanika/DSC_1915.jpg', 'train/saanika/WhatsApp Image 2020-02-08 at 1.32.08 AM.jpg', 'train/saanika/WhatsApp Image 2020-02-08 at 1.32.34 AM.jpg', 'train/saanika/WhatsApp Image 2020-02-08 at 1.33.17 AM.jpg', 'train/saanika/WhatsApp Image 2020-02-08 at 1.35.48 AM.jpg', 'train/saanika/WhatsApp Image 2020-02-08 at 1.38.28 AM.jpg', 'train/saanika/WhatsApp Image 2020-02-08 at 1.40.09 AM.jpg', 'train/saanika/ZDSC_1908.jpg', 'train/saanika/ZDSC_1909.jpg']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "356069b3f7b249cf92928fa84ba2becd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Please change image: train/saanika/DSC_1915.jpg - it has 3 faces; it can only have one\n",
            "Please change image: train/saanika/ZDSC_1908.jpg - it has 2 faces; it can only have one\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-9fd6f49a6c51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'# Get path to all the known images\\n# Filtering on .jpg extension - so this will only work with JPEG images ending with .jpg\\nface_encodings = []\\nnames = []\\nmy_list =[\\'saanika\\',\\'himanshu\\']\\nfor z in my_list:\\n    image_filenames = filter(lambda x: x.endswith(\\'.jpg\\'), os.listdir(\\'/content/gdrive/My Drive/face_recognition/train/\\' + z + \\'/\\'))\\n    # Sort in alphabetical order\\n    image_filenames = sorted(image_filenames)\\n    # Get full paths to images\\n    paths_to_images = [\\'train/\\' + z + \\'/\\' + x for x in image_filenames]\\n    # List of face encodings we have\\n    print(paths_to_images)\\n    # Loop over images to get the encoding one by one\\n    for path_to_image in tqdm_notebook(paths_to_images):\\n        \\n        # Get face encodings from the image\\n        f = 0\\n        face_encodings_in_image = get_face_encodings(path_to_image)\\n        # Make sure there\\'s exactly one face in the image\\n        if len(face_encodings_in_image) != 1:\\n            print(\"Please change image: \" + path_to_image + \" - it has \" + str(len(face_encodings_in_image)) + \" faces; it can only have one\")\\n            f = 1\\n            # exit()\\n        # Append the face encoding found in that image to the list of face encodings we have\\n        if(f == 0):\\n            names.append(path_to_image)\\n           ...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-103-13541299f0ea>\u001b[0m in \u001b[0;36mget_face_encodings\u001b[0;34m(path_to_image)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Detect faces using the face detector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdetected_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Get pose/landmarks of those faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Will be used as an input to the function that computes face encodings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jzibT-qaNF-",
        "colab_type": "code",
        "outputId": "e71ce350-f7aa-47a2-8a15-32655c7eec34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d2f79ab544bd4135994afb209de9737e",
            "0a4481f2cc394aa081b77f2491c65696",
            "f267b444ad1b40a7808ce76ec34f56bd",
            "a2c1e92248da4685a26249289cedc117",
            "806b2dca20b64d869c75aae5fcfa3018",
            "b31a9f4d8d4a4475a9e03e60c39fb5a2",
            "4af6d3143e3d4cfb8c3a330f594030fc",
            "db69991cf7c14d0dbc04ccb758f6cc40"
          ]
        }
      },
      "source": [
        "# Get path to all the test images\n",
        "# Filtering on .jpg extension - so this will only work with JPEG images ending with .jpg\n",
        "test_filenames = filter(lambda x: x.endswith(('.jpg','.jpeg','JPG')), os.listdir('test/'))\n",
        "# Get full paths to test images\n",
        "paths_to_test_images = ['test/' + x for x in test_filenames]\n",
        "# Get list of names of people by eliminating the .JPG extension from image filenames\n",
        "# names = [x[:-4] for x in image_filenames]\n",
        "# Iterate over test images to find match one by one\n",
        "for path_to_image in tqdm_notebook(paths_to_test_images):\n",
        "    # Get face encodings from the test image\n",
        "    face_encodings_in_image = get_face_encodings(path_to_image)\n",
        "    # Make sure there's exactly one face in the image\n",
        "    if len(face_encodings_in_image) == 0:\n",
        "        print(\"Please change image: \" + path_to_image + \" - it has \" + str(len(face_encodings_in_image)) + \" faces; it can only have one\")\n",
        "        # exit()\n",
        "    elif(len(face_encodings_in_image) == 1):\n",
        "    # Find match for the face encoding found in this test image\n",
        "        match = find_match(face_encodings, names, face_encodings_in_image[0])\n",
        "        print(path_to_image, match)\n",
        "    else:\n",
        "        for x in range(len(face_encodings_in_image)):\n",
        "            match = find_match(face_encodings, names, face_encodings_in_image[0])\n",
        "            # Print the path of test image and the corresponding match\n",
        "            print(path_to_image, match)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2f79ab544bd4135994afb209de9737e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=57), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "test/IMG_2834.JPG himanshu\n",
            "\n",
            "test/IMG_2957.JPG himanshu\n",
            "\n",
            "test/IMG_2980.JPG himanshu\n",
            "\n",
            "test/IMG_2986.JPG himanshu\n",
            "\n",
            "test/IMG_2990.JPG himanshu\n",
            "\n",
            "Please change image: test/IMG_20160324_145540417_HDR.jpg - it has 0 faces; it can only have one\n",
            "\n",
            "test/IMG_20160324_151532386.jpg himanshu\n",
            "\n",
            "test/IMG_20160324_151855277 (1).jpg himanshu\n",
            "\n",
            "Please change image: test/IMG_2936_01.jpg - it has 0 faces; it can only have one\n",
            "\n",
            "Please change image: test/IMG_2948_01.jpg - it has 0 faces; it can only have one\n",
            "\n",
            "test/IMG_7444.jpg saanika\n",
            "test/IMG_7444.jpg saanika\n",
            "\n",
            "test/IMG_7448.jpg saanika\n",
            "\n",
            "test/IMG_7450.jpg saanika\n",
            "\n",
            "test/IMG_7568.jpg saanika\n",
            "\n",
            "test/IMG_7571.jpg saanika\n",
            "\n",
            "test/IMG_7578.jpg saanika\n",
            "test/IMG_7578.jpg saanika\n",
            "\n",
            "test/IMG_7579__01.jpg himanshu\n",
            "\n",
            "test/IMG_7655__01.jpg saanika\n",
            "\n",
            "test/IMG_7657__01.jpg saanika\n",
            "\n",
            "test/00000008.jpg Not Found\n",
            "\n",
            "test/00000009.jpg Not Found\n",
            "\n",
            "test/00000003.jpg Not Found\n",
            "\n",
            "test/00000000.jpg Not Found\n",
            "\n",
            "test/DSC_1907.jpg saanika\n",
            "test/DSC_1907.jpg saanika\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-f1073b0112ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath_to_image\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths_to_test_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Get face encodings from the test image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mface_encodings_in_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_face_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Make sure there's exactly one face in the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_encodings_in_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-103-13541299f0ea>\u001b[0m in \u001b[0;36mget_face_encodings\u001b[0;34m(path_to_image)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Detect faces using the face detector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdetected_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Get pose/landmarks of those faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Will be used as an input to the function that computes face encodings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2igCe-J56JA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do5PPA3k60HU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        "while(cap.isOpened()):\n",
        "  # Capture frame-by-frame\n",
        "  ret, frame = cap.read()\n",
        "  if ret == True:\n",
        " \n",
        "    # Display the resulting frame\n",
        "    cv2.imshow('Frame',frame)\n",
        " \n",
        "    # Press Q on keyboard to  exit\n",
        "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "      break\n",
        " \n",
        "  # Break the loop\n",
        "  else: \n",
        "    break\n",
        " \n",
        "# When everything done, release the video capture object\n",
        "cap.release()\n",
        " \n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWfJA5Wh8R92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsr1Tdhm8_em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2laxono9EVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}